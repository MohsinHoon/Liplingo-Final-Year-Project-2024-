{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNMPtxSpdvZGdYmg4ddfHz9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohsinHoon/Liplingo-Final-Year-Project-2024-/blob/main/Final_Year_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2xjQVCiGqyd"
      },
      "outputs": [],
      "source": [
        "!pip install opencv-python matplotlib imageio gdown tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install soundfile"
      ],
      "metadata": {
        "id": "VOmzEMpkLgfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from typing import List\n",
        "from matplotlib import pyplot as plt\n",
        "import imageio\n",
        "import soundfile as sf"
      ],
      "metadata": {
        "id": "t9txKEMnHHCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configurring GPU for access"
      ],
      "metadata": {
        "id": "PSGHZZJQf5w4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ],
      "metadata": {
        "id": "-skOd6JBHJ4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whether the GPU available on Not"
      ],
      "metadata": {
        "id": "xygrmm1ngF-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "try:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "LYdRuL0uHM3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building the Data Loading Function**"
      ],
      "metadata": {
        "id": "NC4FuKcSI0Is"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown"
      ],
      "metadata": {
        "id": "BQPD27adHOoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fetching the dataset directly from the google drive available publcaly\n",
        "Videos with their Alignments"
      ],
      "metadata": {
        "id": "3goQpURUgYQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://drive.google.com/uc?id=1YlvpDLix3S-U8fd-gqRwPcWXAXm8JwjL'\n",
        "output = 'data.zip'\n",
        "gdown.download(url, output, quiet=False)\n",
        "gdown.extractall('data.zip')"
      ],
      "metadata": {
        "id": "l4Rm1sizHRaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qBdfXsmNglt4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the Video File and Pre Processing the annotations we will standanrized the video/images and making the cutout portion"
      ],
      "metadata": {
        "id": "tQXM7uMQg0XB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_video(path:str) -> List[float]:\n",
        "\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "    for _ in range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))):\n",
        "        ret, frame = cap.read()\n",
        "        frame = tf.image.rgb_to_grayscale(frame)\n",
        "        frames.append(frame[190:236,80:220,:])\n",
        "    cap.release()\n",
        "\n",
        "    mean = tf.math.reduce_mean(frames)\n",
        "    std = tf.math.reduce_std(tf.cast(frames, tf.float32))\n",
        "    return tf.cast((frames - mean), tf.float32) / std"
      ],
      "metadata": {
        "id": "QAPu2wOaHfhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining the Vocabulary, here we only define small alphabets as because we dont have any Capital form of data , but we still can add up on these**"
      ],
      "metadata": {
        "id": "gZaOORx4iPny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = [x for x in \"abcdefghijklmnopqrstuvwxyz'?!123456789 \"]"
      ],
      "metadata": {
        "id": "37Y0Qf7ZHkdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we are converting charchters to numbers and numbers to charchters and we are assinging one token to each charchter, i-e: Tokens are be like A=1 and Z=26"
      ],
      "metadata": {
        "id": "K_Rcx3q4igCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_num = tf.keras.layers.StringLookup(vocabulary=vocab, oov_token=\"\")\n",
        "num_to_char = tf.keras.layers.StringLookup(\n",
        "    vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"The vocabulary is: {char_to_num.get_vocabulary()} \"\n",
        "    f\"(size ={char_to_num.vocabulary_size()})\"\n",
        ")"
      ],
      "metadata": {
        "id": "IY93bP37Hlqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Derving the Vocabulary"
      ],
      "metadata": {
        "id": "odxZYD5zjks1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_num.get_vocabulary()"
      ],
      "metadata": {
        "id": "RUKW9ucPHoRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Token can visible on this LOC**"
      ],
      "metadata": {
        "id": "cxE9OWpnjs2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_num(['l','i','p','l' ,'i', 'n', 'g', 'o' ])"
      ],
      "metadata": {
        "id": "ycnC7Cu3Hqnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**These show where the above charachter is on which number**"
      ],
      "metadata": {
        "id": "EUmatFzojyoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_to_char([12,9,16,12,9,14,7,15])"
      ],
      "metadata": {
        "id": "DsZFI8C3IJGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Here, we are loading the Alingments for the video like where the as the sound and lip movements matches the Video, Thats why it is difficult to make a custom dataset because we are unable to figure the alingmnets* bold text"
      ],
      "metadata": {
        "id": "K5veWM_NkD8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_alignments(path:str) -> List[str]:\n",
        "    with open(path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    tokens = []\n",
        "    for line in lines:\n",
        "        line = line.split()\n",
        "        if line[2] != 'sil':\n",
        "            tokens = [*tokens,' ',line[2]]\n",
        "    return char_to_num(tf.reshape(tf.strings.unicode_split(tokens, input_encoding='UTF-8'), (-1)))[1:]"
      ],
      "metadata": {
        "id": "FJ55mY-mIafq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing the Path**"
      ],
      "metadata": {
        "id": "EL4Fvcu-kdVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_path = '.\\\\data\\\\s1\\\\bbal6n.mpg'"
      ],
      "metadata": {
        "id": "hfvpIQejIdUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Through .tf we decoding the numpy array as we convert on above code**"
      ],
      "metadata": {
        "id": "sB_waT_VkhnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.convert_to_tensor(test_path).numpy().decode('utf-8').split('\\\\')[-1].split('.')[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tXPwSI3sIf9H",
        "outputId": "8e38dfc4-bbb8-42c3-c530-f4076817b1cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bbal6n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    }
  ]
}